from flask import Flask, render_template, request, jsonify
import sys
import os
import json
import numpy as np

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from indexing.index_builder import IndexBuilder
from text_preprocessing.preprocessor_factory import PreprocessorFactory
from vector_storage.chroma_storage import ChromaStorage
from documents_processing.collector import DocumentCollector  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
from text_preprocessing.batching import BatchTextPreprocessor  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
from .json_utils import safe_json_response, CustomJSONEncoder


class SearchApp:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º"""

    def __init__(self):
        self.app = Flask(__name__, 
                        template_folder='templates',
                        static_folder='static')
        self.app.config['SECRET_KEY'] = 'search-system-secret-key'
        self.app.json_encoder = CustomJSONEncoder
        self.index_builder = None
        self.preprocessor = None
        self.is_loaded = False
        self.all_documents = []  # –•—Ä–∞–Ω–∏–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

        self.setup_routes()
        self.load_search_system()

    def load_search_system(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º"""
        try:
            print("–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º...")

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self.preprocessor = PreprocessorFactory.create_lemmatization_preprocessor()


            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
                self.index_builder = IndexBuilder(
                    use_vector_db=True,
                    use_document_selector=True,  # –í–ö–õ–Æ–ß–ê–ï–ú —Å–µ–ª–µ–∫—Ç–æ—Ä!
                    use_semantic_search=True    # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –ø–æ–∑–∂–µ
                )
                
                
                self.index_builder.vocabulary.load_vocabulary("search_index/vocabulary.json")
                self.index_builder.vector_storage = ChromaStorage()
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º TF-IDF –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä
                from indexing.tfidf_calculator import TFIDFCalculator
                self.index_builder.tfidf_calculator = TFIDFCalculator(self.index_builder.vocabulary)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
                self._load_documents_for_selector()
                
                self.is_loaded = True
                print("‚úÖ –ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
            except Exception as e:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å: {e}")
                print("üîÑ –ü—Ä–æ–±—É–µ–º –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Å –Ω—É–ª—è...")
                self._build_index_from_scratch()

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            self.is_loaded = False

    def _load_documents_for_selector(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–µ–ª–µ–∫—Ç–æ—Ä–∞"""
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏ docs
            collector = DocumentCollector()
            self.all_documents = collector.collect_documents("docs", recursive=True)
            
            if self.all_documents:
                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
                batch_processor = BatchTextPreprocessor(self.preprocessor)
                batch_processor.preprocess_collection(self.all_documents)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ index_builder –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
                self.index_builder.all_documents = self.all_documents
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.all_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞")
            else:
                print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞: {e}")

    def _build_index_from_scratch(self):
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å —Å –Ω—É–ª—è"""
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            collector = DocumentCollector()
            documents = collector.collect_documents("docs", recursive=True)
            
            if not documents:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
                return
                
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            batch_processor = BatchTextPreprocessor(self.preprocessor)
            batch_processor.preprocess_collection(documents)
            
            # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º
            self.index_builder = IndexBuilder(
                use_vector_db=True,
                use_document_selector=True,
                use_semantic_search=False
            )
            self.index_builder.build_index(documents)
            self.index_builder.save_index("search_index")
            
            self.all_documents = documents
            self.is_loaded = True
            print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")

    def setup_routes(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤ Flask"""

        @self.app.route('/')
        def index():
            """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –ø–æ–∏—Å–∫–æ–º"""
            total_docs = 0
            selection_stats = {}
            
            if self.index_builder and self.index_builder.vector_storage:
                total_docs = self.index_builder.vector_storage.get_document_count()
                
            if self.index_builder and self.index_builder.document_selector:
                selection_stats = self.index_builder.document_selector.get_selection_statistics()

            return render_template('index.html',
                                   system_loaded=self.is_loaded,
                                   total_documents=total_docs,
                                   selection_stats=selection_stats)

        @self.app.route('/search', methods=['POST'])
        def search():
            """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º"""
            if not self.is_loaded:
                return jsonify({'error': '–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}), 500

            try:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ —Ñ–æ—Ä–º—ã
                query = request.form.get('query', '').strip()
                top_k = int(request.form.get('top_k', 10))
                show_analysis = request.form.get('show_analysis', 'false') == 'true'

                if not query:
                    return jsonify({'error': '–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å'}), 400

                print(f"üîç –ü–æ–∏—Å–∫ –∑–∞–ø—Ä–æ—Å–∞ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: '{query}'")

                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ (—Ç–µ–ø–µ—Ä—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä)
                results = self.index_builder.search(query, self.preprocessor, top_k=top_k)

                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
                selection_stats = {}
                expansion_result = {}
                
                if self.index_builder.document_selector:
                    selection_stats = self._safe_serialize_stats(
                        self.index_builder.document_selector.get_selection_statistics()
                    )
                    expansion_result = self._safe_serialize_expansion(
                        self.index_builder.document_selector.get_last_expansion_result()
                    )

                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                formatted_results = []
                for result in results:
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    safe_result = self._safe_serialize_result(result)
                    formatted_results.append(safe_result)

                response_data = {
                    'query': query,
                    'total_found': len(results),
                    'results': formatted_results,
                    'selection_stats': selection_stats,
                    'expansion_result': expansion_result
                }

                # –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
                if show_analysis:
                    query_analysis = self.index_builder.analyze_query(query, self.preprocessor)
                    response_data['query_analysis'] = self._safe_serialize_analysis(query_analysis)

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é
                return safe_json_response(response_data)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                import traceback
                traceback.print_exc()
                return jsonify({'error': f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}'}), 500

        @self.app.route('/selection-stats')
        def selection_stats():
            """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã —Å–µ–ª–µ–∫—Ç–æ—Ä–∞"""
            if not self.is_loaded or not self.index_builder.document_selector:
                return jsonify({'error': '–°–µ–ª–µ–∫—Ç–æ—Ä –Ω–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω'}), 500

            stats = self.index_builder.document_selector.get_selection_statistics()
            return safe_json_response(self._safe_serialize_stats(stats))

        @self.app.route('/analyze-query', methods=['POST'])
        def analyze_query():
            """–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –±–µ–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
            if not self.is_loaded:
                return jsonify({'error': '–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}), 500

            try:
                query = request.form.get('query', '').strip()
                if not query:
                    return jsonify({'error': '–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å'}), 400

                analysis = self.index_builder.analyze_query(query, self.preprocessor)
                return safe_json_response(self._safe_serialize_analysis(analysis))

            except Exception as e:
                return jsonify({'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}'}), 500

        @self.app.route('/stats')
        def stats():
            """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
            if not self.is_loaded:
                return jsonify({'error': '–°–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}), 500

            stats = self.index_builder.get_index_statistics()
            return safe_json_response(self._safe_serialize_stats(stats))

        @self.app.route('/health')
        def health():
            """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
            total_docs = 0
            if self.index_builder and self.index_builder.vector_storage:
                total_docs = self.index_builder.vector_storage.get_document_count()

            return jsonify({
                'status': 'ready' if self.is_loaded else 'loading',
                'documents_loaded': total_docs
            })

        @self.app.route('/debug-query', methods=['POST'])
        def debug_query():
            """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
            if not self.is_loaded:
                return jsonify({'error': '–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}), 500

            try:
                query = request.form.get('query', '').strip()
                if not query:
                    return jsonify({'error': '–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å'}), 400

                # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞
                analysis = self.index_builder.tfidf_calculator.debug_query_processing(query, self.preprocessor)

                return jsonify({
                    'query': query,
                    'debug_info': '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Å–æ–ª—å —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏'
                })

            except Exception as e:
                return jsonify({'error': f'–û—à–∏–±–∫–∞ –æ—Ç–ª–∞–¥–∫–∏: {str(e)}'}), 500

        @self.app.route('/vocabulary-stats')
        def vocabulary_stats():
            """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ª–æ–≤–∞—Ä—è"""
            if not self.is_loaded:
                return jsonify({'error': '–°–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}), 500

            vocab = self.index_builder.vocabulary
            stats = vocab.get_statistics()

            # –ü—Ä–∏–º–µ—Ä—ã —Ç–µ—Ä–º–∏–Ω–æ–≤
            sample_terms = list(vocab.term_to_index.keys())[:50]

            return jsonify({
                'vocabulary_size': stats['vocabulary_size'],
                'total_documents': stats['total_documents'],
                'sample_terms': sample_terms,
                'most_frequent_terms': stats['most_frequent_terms'][:20]
            })
        
    def _safe_serialize_stats(self, stats):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not stats:
            return {}
        
        safe_stats = {}
        for key, value in stats.items():
            if value is None:
                safe_stats[key] = None
            elif isinstance(value, (int, str, bool)):
                safe_stats[key] = value
            elif isinstance(value, (float, np.float32, np.float64)):
                safe_stats[key] = float(value)
            elif isinstance(value, dict):
                safe_stats[key] = self._safe_serialize_stats(value)
            elif isinstance(value, list):
                safe_stats[key] = [self._safe_serialize_stats(item) if isinstance(item, dict) else item for item in value]
            else:
                safe_stats[key] = str(value)
        
        return safe_stats

    def _safe_serialize_expansion(self, expansion_result):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        if not expansion_result:
            return {
                'original_terms': [],
                'expanded_terms': [],
                'similar_terms': {},
                'all_terms': [],
                'expansion_ratio': 1.0
            }
        
        safe_expansion = {}
        for key, value in expansion_result.items():
            if key == 'similar_terms' and isinstance(value, dict):
                # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º similar_terms
                safe_similar = {}
                for term, similar_list in value.items():
                    safe_similar[term] = [
                        (similar_word, float(score)) 
                        for similar_word, score in similar_list
                    ]
                safe_expansion[key] = safe_similar
            elif key == 'expansion_ratio':
                safe_expansion[key] = float(value)
            elif isinstance(value, list):
                safe_expansion[key] = [str(item) for item in value]
            else:
                safe_expansion[key] = value
        
        return safe_expansion

    def _safe_serialize_result(self, result):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞"""
        if not result:
            return {}
        
        safe_result = {}
        for key, value in result.items():
            if key == 'similarity_score':
                safe_result[key] = float(value)
            elif key == 'metadata' and isinstance(value, dict):
                safe_result[key] = self._safe_serialize_stats(value)
            elif key == 'semantic_info' and isinstance(value, dict):
                safe_semantic = {}
                for sem_key, sem_value in value.items():
                    if sem_key in ['original_score', 'semantic_score', 'combined_score']:
                        safe_semantic[sem_key] = float(sem_value)
                    elif sem_key == 'expansion_result':
                        safe_semantic[sem_key] = self._safe_serialize_expansion(sem_value)
                    else:
                        safe_semantic[sem_key] = sem_value
                safe_result[key] = safe_semantic
            elif key == 'distance':
                safe_result[key] = float(value) if value is not None else None
            else:
                safe_result[key] = value
        
        return safe_result

    def _safe_serialize_analysis(self, analysis):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        if not analysis:
            return {}
        
        safe_analysis = {}
        for key, value in analysis.items():
            if key == 'term_analysis' and isinstance(value, list):
                safe_terms = []
                for term_info in value:
                    safe_term = {}
                    for term_key, term_value in term_info.items():
                        if term_key in ['idf', 'weight_in_query']:
                            safe_term[term_key] = float(term_value)
                        else:
                            safe_term[term_key] = term_value
                    safe_terms.append(safe_term)
                safe_analysis[key] = safe_terms
            else:
                safe_analysis[key] = value
        
        return safe_analysis

    def _generate_snippet(self, text: str, query: str, max_length: int = 200) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–Ω–∏–ø–ø–µ—Ç–∞ —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π –∑–∞–ø—Ä–æ—Å–∞"""
        if not text:
            return ""

        query_terms = query.lower().split()
        text_lower = text.lower()

        # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –ª—é–±–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞
        best_position = len(text)
        for term in query_terms:
            pos = text_lower.find(term)
            if pos != -1 and pos < best_position:
                best_position = pos

        # –í—ã—Ä–µ–∑–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
        start = max(0, best_position - 50)
        end = min(len(text), start + max_length)

        snippet = text[start:end]

        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω
        if start > 0:
            snippet = "..." + snippet
        if end < len(text):
            snippet = snippet + "..."

        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
        for term in query_terms:
            snippet = self._highlight_term(snippet, term)

        return snippet

    def _highlight_term(self, text: str, term: str) -> str:
        """–ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ"""
        import re
        pattern = re.compile(re.escape(term), re.IGNORECASE)
        return pattern.sub(f'<mark>{term}</mark>', text)

    def _find_query_terms(self, query: str, text: str) -> list:
        """–ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Ç–µ–∫—Å—Ç–µ"""
        query_terms = set(query.lower().split())
        text_terms = set(text.lower().split())
        return list(query_terms & text_terms)

    def run(self, host='127.0.0.1', port=5000, debug=True):
        """–ó–∞–ø—É—Å–∫ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞"""
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –Ω–∞ http://{host}:{port}")
        if self.is_loaded:
            total_docs = self.index_builder.vector_storage.get_document_count()
            print(f"üìä –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –ø–æ–∏—Å–∫—É! –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {total_docs}")
        else:
            print("‚ö†Ô∏è  –°–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞.")

        self.app.run(host=host, port=port, debug=debug)