from typing import List, Dict
import numpy as np
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from .base_selector import BaseDocumentSelector


class SemanticEnhancer(BaseDocumentSelector):
    """
    –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ Word2Vec
    —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–∞ –∏ –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–≤
    """

    def __init__(self, word2vec_model_path: str = None, similarity_threshold: float = 0.6):
        super().__init__("SemanticEnhancer")
        self.similarity_threshold = float(similarity_threshold)  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º float
        self.word_vectors = None
        self.vocabulary = set()
        
        if word2vec_model_path:
            self.load_word2vec_model(word2vec_model_path)
        else:
            self._create_demo_model()

    def load_word2vec_model(self, model_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Word2Vec"""
        try:
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ Word2Vec –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
            self.word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)
            self.vocabulary = set(self.word_vectors.key_to_index.keys())
            print(f"‚úÖ Word2Vec –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.vocabulary)}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Word2Vec –º–æ–¥–µ–ª–∏: {e}")

    def expand_query_with_similar_words(self, query: str, top_n: int = 3) -> Dict:
        """
        –†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —Å–ª–æ–≤–∞—Ä—å –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤
        """
        if not self.word_vectors:
            return {
                'original_terms': query.split(),
                'expanded_terms': query.split(),
                'similar_terms': {},
                'all_terms': query.split(),
                'expansion_ratio': 1.0
            }

        original_terms = [term.lower().strip() for term in query.split() if term.strip()]
        expanded_terms = original_terms.copy()
        similar_terms = {}
        
        print(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")

        for term in original_terms:
            if term in self.vocabulary:
                try:
                    similar_words = self.word_vectors.most_similar(term, topn=top_n)
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
                    filtered_similar = [
                        (similar_word, float(similarity))  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
                        for similar_word, similarity in similar_words 
                        if similarity >= self.similarity_threshold and similar_word != term
                    ]
                    
                    if filtered_similar:
                        similar_terms[term] = filtered_similar
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                        for similar_word, similarity in filtered_similar:
                            if similar_word not in expanded_terms:
                                expanded_terms.append(similar_word)
                                
                        print(f"   üìñ '{term}': {[f'{word}({sim:.2f})' for word, sim in filtered_similar]}")
                    else:
                        print(f"   ‚ö†Ô∏è  –î–ª—è '{term}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤")
                        
                except KeyError:
                    print(f"   ‚ùå –°–ª–æ–≤–æ '{term}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –º–æ–¥–µ–ª–∏ Word2Vec")
            else:
                print(f"   ‚ö†Ô∏è  –°–ª–æ–≤–æ '{term}' –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ Word2Vec")

        # –í—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ + —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        all_search_terms = list(set(original_terms + expanded_terms))
        expansion_ratio = len(expanded_terms) / len(original_terms) if original_terms else 1.0

        return {
            'original_terms': original_terms,
            'expanded_terms': expanded_terms,
            'similar_terms': similar_terms,
            'all_terms': all_search_terms,
            'expansion_ratio': float(expansion_ratio)  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º float
        }

    def highlight_semantic_terms(self, text: str, expansion_result: Dict) -> str:
        """
        –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        """
        if not text:
            return ""

        highlighted_text = text
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏: —Ç–µ—Ä–º–∏–Ω -> —Ç–∏–ø –ø–æ–¥—Å–≤–µ—Ç–∫–∏
        highlight_terms = {}
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã - –∑–µ–ª–µ–Ω—ã–π —Ü–≤–µ—Ç
        for term in expansion_result['original_terms']:
            if len(term) > 2:  # –¢–æ–ª—å–∫–æ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª–∏–Ω–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤
                highlight_terms[term] = 'semantic-original'
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã - —Å–∏–Ω–∏–π —Ü–≤–µ—Ç
        for original_term, similar_list in expansion_result['similar_terms'].items():
            for similar_term, similarity in similar_list:
                if len(similar_term) > 2:
                    highlight_terms[similar_term] = 'semantic-expanded'

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã –ø–æ –¥–ª–∏–Ω–µ (—Å–Ω–∞—á–∞–ª–∞ –¥–ª–∏–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π)
        sorted_terms = sorted(highlight_terms.keys(), key=len, reverse=True)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–¥—Å–≤–µ—Ç–∫—É
        for term in sorted_terms:
            if term.lower() in highlighted_text.lower():
                color_class = highlight_terms[term]
                if color_class == 'semantic-original':
                    highlight_html = f'<mark class="semantic-original" title="–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ä–º–∏–Ω –∑–∞–ø—Ä–æ—Å–∞">{term}</mark>'
                else:
                    highlight_html = f'<mark class="semantic-expanded" title="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–π —Ç–µ—Ä–º–∏–Ω">{term}</mark>'
                
                # –ó–∞–º–µ–Ω—è–µ–º —Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞
                import re
                pattern = re.compile(re.escape(term), re.IGNORECASE)
                highlighted_text = pattern.sub(highlight_html, highlighted_text)

        return highlighted_text

    def calculate_semantic_similarity(self, query: str, document) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
        """
        if not self.word_vectors or not hasattr(document, 'processed_content'):
            return 0.0

        # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
        expansion_result = self.expand_query_with_similar_words(query)
        all_search_terms = expansion_result['all_terms']
        
        if not all_search_terms:
            return 0.0

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_terms = set(document.processed_content.split())
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        semantic_score = 0.0
        matched_terms = 0

        for query_term in all_search_terms:
            if query_term in doc_terms:
                # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                semantic_score += 1.0
                matched_terms += 1
            elif query_term in self.vocabulary:
                # –ò—â–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                best_similarity = 0.0
                for doc_term in doc_terms:
                    if doc_term in self.vocabulary:
                        try:
                            similarity = float(self.word_vectors.similarity(query_term, doc_term))  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                            if similarity > best_similarity:
                                best_similarity = similarity
                        except KeyError:
                            continue
                
                if best_similarity >= self.similarity_threshold:
                    semantic_score += best_similarity
                    matched_terms += 1

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∫–æ—Ä
        if matched_terms > 0:
            semantic_score = semantic_score / len(all_search_terms)
            
        return float(min(semantic_score, 1.0))  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º float

    def enhance_search_with_semantics(self, query: str, search_results: List[Dict], 
                                    documents: List) -> List[Dict]:
        """
        –£–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏
        """
        print("üéØ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–∞...")

        # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
        expansion_result = self.expand_query_with_similar_words(query)
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ doc_id -> –¥–æ–∫—É–º–µ–Ω—Ç
        doc_map = {doc.doc_id: doc for doc in documents}
        
        enhanced_results = []
        
        for result in search_results:
            doc_id = result['metadata']['doc_id']
            document = doc_map.get(doc_id)
            
            if document:
                # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–æ—Ä
                semantic_score = self.calculate_semantic_similarity(query, document)
                
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Å–∫–æ—Ä–æ–º
                original_score = float(result['similarity_score'])  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                combined_score = self._combine_scores(original_score, semantic_score)
                
                # –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –≤ —Å–Ω–∏–ø–ø–µ—Ç–µ
                original_snippet = result.get('snippet', '')
                highlighted_snippet = self.highlight_semantic_terms(original_snippet, expansion_result)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                enhanced_result = result.copy()
                enhanced_result['similarity_score'] = float(combined_score)  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º float
                enhanced_result['semantic_info'] = {
                    'original_score': float(original_score),
                    'semantic_score': float(semantic_score),
                    'combined_score': float(combined_score),
                    'expansion_result': expansion_result,
                    'highlighted_snippet': highlighted_snippet
                }
                enhanced_result['snippet'] = highlighted_snippet  # –ó–∞–º–µ–Ω—è–µ–º —Å–Ω–∏–ø–ø–µ—Ç
                
                enhanced_results.append(enhanced_result)
            else:
                enhanced_results.append(result)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–ª—É—á—à–µ–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É
        enhanced_results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        self.stats = {
            'semantically_enhanced': len(enhanced_results),
            'avg_semantic_score': float(sum(r['semantic_info']['semantic_score'] for r in enhanced_results) / len(enhanced_results)) if enhanced_results else 0.0,
            'query_expansion_ratio': float(expansion_result['expansion_ratio'])
        }
        
        return enhanced_results

    def _combine_scores(self, original_score: float, semantic_score: float) -> float:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ TF-IDF –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–æ—Ä–∞
        """
        return float(0.7 * original_score + 0.3 * semantic_score)  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º float

    def select_documents(self, query: str, documents: List, top_k: int = 10) -> List:
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞"""
        scored_docs = []
        
        for doc in documents:
            semantic_score = self.calculate_semantic_similarity(query, doc)
            scored_docs.append((semantic_score, doc))
        
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        return [doc for score, doc in scored_docs[:top_k]]

    def get_semantic_analysis(self, query: str, document) -> Dict:
        """
        –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        expansion_result = self.expand_query_with_similar_words(query)
        
        analysis = {
            'query': query,
            'original_terms': expansion_result['original_terms'],
            'expanded_terms': expansion_result['expanded_terms'],
            'similar_terms': expansion_result['similar_terms'],
            'document_terms_count': len(document.processed_content.split()),
            'semantic_score': self.calculate_semantic_similarity(query, document)
        }
        
        return analysis