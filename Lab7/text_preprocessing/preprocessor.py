# text_preprocessing/preprocessor.py
import re
import string
from .nltk_setup import download_nltk_resources
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag as nltk_pos_tag  
from typing import List, Dict


class TextPreprocessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ
    """

    def __init__(self, use_lemmatization=True, custom_stopwords=None):
        self.use_lemmatization = use_lemmatization
        self.stop_words = set(stopwords.words('english'))

        if custom_stopwords:
            self.stop_words.update(custom_stopwords)

        if self.use_lemmatization:
            self.lemmatizer = WordNetLemmatizer()
        else:
            self.lemmatizer = None

    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""

        text = text.lower()
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã
        text = text.translate(str.maketrans('', '', string.punctuation + string.digits))
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def get_wordnet_pos(self, treebank_tag):
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–≥–∏ Treebank –≤ —Ç–µ–≥–∏ WordNet"""
        if treebank_tag.startswith('J'):
            return 'a'  # adjective
        elif treebank_tag.startswith('V'):
            return 'v'  # verb
        elif treebank_tag.startswith('N'):
            return 'n'  # noun
        elif treebank_tag.startswith('R'):
            return 'r'  # adverb
        else:
            return 'n'  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é noun

    def smart_lemmatize(self, tokens):
        """–£–º–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏"""
        if not self.lemmatizer:
            return tokens

        # –ü–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        pos_tags = nltk_pos_tag(tokens)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç
        lemmatized_tokens = []

        for token, tag in pos_tags:  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º tag –≤–º–µ—Å—Ç–æ pos_tag
            wordnet_pos = self.get_wordnet_pos(tag)
            lemma = self.lemmatizer.lemmatize(token, pos=wordnet_pos)
            lemmatized_tokens.append(lemma)

            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –≥–ª–∞–≥–æ–ª–æ–≤
            if tag.startswith('V') and token != lemma:
                print(f"   üîÑ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≥–ª–∞–≥–æ–ª–∞: '{token}' -> '{lemma}' (POS: {tag})")

        return lemmatized_tokens

    def preprocess_text(self, text: str, return_string: bool = True, debug: bool = False):
        """–ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π"""
        if not text:
            return "" if return_string else []

        if debug:
            print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '{text}'")

        # –û—á–∏—Å—Ç–∫–∞
        cleaned_text = self.clean_text(text)
        if debug:
            print(f"üìù –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: '{cleaned_text}'")

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = word_tokenize(cleaned_text)
        if debug:
            print(f"üî§ –¢–æ–∫–µ–Ω—ã: {tokens}")

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]
        if debug:
            print(f"üö´ –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤: {tokens}")

        # –£–º–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏
        if self.use_lemmatization and self.lemmatizer:
            if debug:
                print("üß† –ü—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω—É—é –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é...")
            tokens = self.smart_lemmatize(tokens)
            if debug:
                print(f"‚úÖ –ü–æ—Å–ª–µ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {tokens}")

        if return_string:
            result = ' '.join(tokens)
            if debug:
                print(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: '{result}'")
            return result
        else:
            if debug:
                print(f"üéØ –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {tokens}")
            return tokens

    def preprocess_document(self, document) -> Dict:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not document or not document.content:
            return {
                'original_content': '',
                'processed_content': '',
                'tokens': [],
                'token_count': 0,
                'unique_tokens': set()
            }

        original_content = document.content
        processed_content = self.preprocess_text(original_content, return_string=True, debug=False)
        tokens = self.preprocess_text(original_content, return_string=False, debug=False)

        document.processed_content = processed_content

        return {
            'original_content': original_content[:200] + '...' if len(original_content) > 200 else original_content,
            'processed_content': processed_content,
            'tokens': tokens,
            'token_count': len(tokens),
            'unique_tokens': set(tokens)
        }

    def debug_term(self, term: str):
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞"""
        print(f"\nüîç –û–¢–õ–ê–î–ö–ê –¢–ï–†–ú–ò–ù–ê: '{term}'")
        print("=" * 40)

        result = self.preprocess_text(term, return_string=True, debug=True)
        return result