# main.py
import sys
import os
import argparse
from documents_processing.collector import DocumentCollector
from text_preprocessing.preprocessor_factory import PreprocessorFactory
from text_preprocessing.batching import BatchTextPreprocessor
from indexing.index_builder import IndexBuilder
from web_interface.app import SearchApp


def build_search_index(docs_directory: str = "docs"):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
    print("=== –ü–û–°–¢–†–û–ï–ù–ò–ï –ü–û–ò–°–ö–û–í–û–ì–û –ò–ù–î–ï–ö–°–ê ===")

    # 1. –°–±–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("\n1. –°–ë–û–† –î–û–ö–£–ú–ï–ù–¢–û–í")
    collector = DocumentCollector()
    documents = collector.collect_documents(docs_directory)

    if not documents:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return None

    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    print("\n2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–û–í")
    preprocessor = PreprocessorFactory.create_lemmatization_preprocessor()
    batch_processor = BatchTextPreprocessor(preprocessor)
    batch_processor.preprocess_collection(documents)
    batch_processor.print_statistics()

    # 3. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
    print("\n3. –ü–û–°–¢–†–û–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê")
    index_builder = IndexBuilder(use_vector_db=True, use_semantic_search=True)
    index_builder.build_index(documents)

    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    print("\n4. –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê")
    index_builder.save_index("search_index")

    # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n5. –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    index_builder.print_detailed_statistics()

    return index_builder


def run_web_interface(host='127.0.0.1', port=5000, debug=True):
    """–ó–∞–ø—É—Å–∫ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    print("=== –ó–ê–ü–£–°–ö –í–ï–ë-–ò–ù–¢–ï–†–§–ï–ô–°–ê ===")

    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    search_app = SearchApp()
    search_app.run(host=host, port=port, debug=debug)

def test_search_system():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º"""
    print("=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–ò–°–ö–û–í–û–ô –°–ò–°–¢–ï–ú–´ –° –ì–ò–ë–†–ò–î–ù–´–ú –°–ï–õ–ï–ö–¢–û–†–û–ú ===")

    try:
        from text_preprocessing.preprocessor_factory import PreprocessorFactory
        from indexing.index_builder import IndexBuilder
        from documents_processing.collector import DocumentCollector
        from text_preprocessing.batching import BatchTextPreprocessor
        from vector_storage.chroma_storage import ChromaStorage

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        collector = DocumentCollector()
        documents = collector.collect_documents("docs", recursive=True)
        
        if not documents:
            print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            return

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        preprocessor = PreprocessorFactory.create_lemmatization_preprocessor()
        batch_processor = BatchTextPreprocessor(preprocessor)
        batch_processor.preprocess_collection(documents)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –° —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º
        index_builder = IndexBuilder(
            use_vector_db=True,
            use_document_selector=True,  # –í–ö–õ–Æ–ß–ê–ï–ú —Å–µ–ª–µ–∫—Ç–æ—Ä!
            use_semantic_search=False
        )
        
        try:
            index_builder.vocabulary.load_vocabulary("search_index/vocabulary.json")
            index_builder.vector_storage = ChromaStorage()
            from indexing.tfidf_calculator import TFIDFCalculator
            index_builder.tfidf_calculator = TFIDFCalculator(index_builder.vocabulary)
            index_builder.all_documents = documents  
        except:
            print("‚ö†Ô∏è  –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å—Ç—Ä–æ–∏–º —Å –Ω—É–ª—è...")
            index_builder.build_index(documents)
            index_builder.save_index("search_index")

        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        test_queries = [
            "computer science and artificial intelligence",
            "data analysis and machine learning", 
            "programming and development"
        ]

        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞:")
        for query in test_queries:
            print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")

            # –ü–æ–∏—Å–∫ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º
            results = index_builder.search(query, preprocessor, top_k=3)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
            if index_builder.document_selector:
                stats = index_builder.document_selector.get_selection_statistics()
                print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞: {stats}")

            if results:
                for i, result in enumerate(results, 1):
                    print(f"  {i}. {result['metadata']['title']} " 
                          f"(—Å—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity_score']:.3f})")
            else:
                print("  ‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

    
def test_document_selector():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è –æ—Ç–±–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    print("=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–£–õ–Ø –û–¢–ë–û–†–ê –î–û–ö–£–ú–ï–ù–¢–û–í ===")

    try:
        from document_selector.hybrid_selector import HybridDocumentSelector
        from document_selector.rule_based_selector import RuleBasedSelector
        from text_preprocessing.preprocessor_factory import PreprocessorFactory

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞
        collector = DocumentCollector()
        documents = collector.collect_documents("docs", recursive=True)

        if not documents:
            print("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            return

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        preprocessor = PreprocessorFactory.create_lemmatization_preprocessor()
        batch_processor = BatchTextPreprocessor(preprocessor)
        batch_processor.preprocess_collection(documents)

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º RuleBasedSelector
        print("\n1. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RuleBasedSelector:")
        rule_selector = RuleBasedSelector()
        test_query = "computer science data"

        selected_docs = rule_selector.select_documents(test_query, documents, top_k=5)
        print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(selected_docs)}")
        print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç–±–æ—Ä–∞:", rule_selector.get_selection_stats())

        for i, doc in enumerate(selected_docs, 1):
            explanation = rule_selector.explain_selection(test_query, doc)
            print(f"  {i}. {doc.title} (score: {explanation['total_score']:.2f})")

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if selected_docs:
            first_doc = selected_docs[0]
            detailed_explanation = rule_selector.explain_selection(test_query, first_doc)
            print(f"\n–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è '{first_doc.title}':")
            for factor in detailed_explanation['factors']:
                print(f"  - {factor['factor']}: {factor['score']:.2f} * {factor['weighted_score']:.2f}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞: {e}")

def main():
    parser = argparse.ArgumentParser(description='–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-–ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞')
    parser.add_argument('--build-index', action='store_true',
                        help='–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å')
    parser.add_argument('--web', action='store_true',
                        help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å')
    parser.add_argument('--use-word2vec', action='store_true',
                        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Word2Vec –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--word2vec-model', default='models/word2vec.bin',
                        help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ Word2Vec')
    parser.add_argument('--test', action='store_true',
                        help='–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É')
    parser.add_argument('--test-selector', action='store_true',
                        help='–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å –æ—Ç–±–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--host', default='127.0.0.1',
                        help='–•–æ—Å—Ç –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 127.0.0.1)')
    parser.add_argument('--port', type=int, default=5000,
                        help='–ü–æ—Ä—Ç –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5000)')
    parser.add_argument('--docs', default='docs',
                        help='–ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: docs)')

    args = parser.parse_args()

    print("=== –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û-–ü–û–ò–°–ö–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê ===")
    print("–í–∞—Ä–∏–∞–Ω—Ç 33: –í–µ–∫—Ç–æ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å, –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫")
    print("–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ #7 - –ë–ì–£–ò–† 2024")
    print()

    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ø—Ä–∞–≤–∫—É
    if not any([args.build_index, args.web, args.test]):
        parser.print_help()
        return

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    if args.build_index:
        build_search_index(args.docs)
        print("\n" + "=" * 50)

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    if args.test:
        test_search_system()
        print("\n" + "=" * 50)

    if args.test_selector:
        test_document_selector()
        print("\n" + "=" * 50)

    # –ó–∞–ø—É—Å–∫ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    if args.web:
        run_web_interface(host=args.host, port=args.port)

    if args.use_word2vec:
        print("üîç –ê–∫—Ç–∏–≤–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ Word2Vec...")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
        if not os.path.exists(args.word2vec_model):
            print("‚ùå –ú–æ–¥–µ–ª—å Word2Vec –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é...")
            from scripts.download_model import download_word2vec_model
            args.word2vec_model = download_word2vec_model()


if __name__ == '__main__':
    main()