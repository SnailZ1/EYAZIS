from typing import List, Dict
import json
from .vocabulary import Vocabulary
from .tfidf_calculator import TFIDFCalculator
from vector_storage.chroma_storage import ChromaStorage
from document_selector.hybrid_selector import HybridDocumentSelector

class IndexBuilder:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
    """

    def __init__(self, use_vector_db: bool = True, use_document_selector: bool = True,
                 use_semantic_search: bool = True, word2vec_model_path: str = 'models/glove-wiki-gigaword-200.bin'):
        self.vocabulary = Vocabulary()
        self.tfidf_calculator = None
        self.tfidf_vectors = {}
        self.use_vector_db = use_vector_db
        self.vector_storage = None
        self.document_selector = None
        self.all_documents = []  # –î–æ–±–∞–≤–ª—è–µ–º —Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        if use_vector_db:
            self.vector_storage = ChromaStorage()

        print('–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ!')

        if use_document_selector:
            self.document_selector = HybridDocumentSelector(
                use_pre_selection=True,
                use_ranking_enhancement=True,
                use_semantic_search=use_semantic_search,
                word2vec_model_path=word2vec_model_path
            )

        print('–ì–∏–±—Ä–∏–¥–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–Ω!')

    def semantic_query_analysis(self, query: str) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
        """
        if not self.document_selector:
            return {'error': 'Document selector not initialized'}
        
        return self.document_selector.semantic_query_expansion(query)

    def build_index(self, documents: List) -> None:
        """
        –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞:
        1. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        2. –†–∞—Å—á–µ—Ç TF-IDF –≤–µ—Å–æ–≤
        3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        """
        print("=== –ù–ê–ß–ê–õ–û –ü–û–°–¢–†–û–ï–ù–ò–Ø –ò–ù–î–ï–ö–°–ê ===")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å–µ–ª–µ–∫—Ç–æ—Ä–µ
        self.all_documents = documents

        # 1. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        self.vocabulary.build_from_documents(documents)

        # 2. –†–∞—Å—á–µ—Ç TF-IDF –≤–µ—Å–æ–≤
        self.tfidf_calculator = TFIDFCalculator(self.vocabulary)
        self.tfidf_vectors = self.tfidf_calculator.calculate_tfidf_weights(documents)

        # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
        if self.use_vector_db and self.vector_storage:
            self.vector_storage.store_documents(documents, self.tfidf_vectors)

        print("=== –ü–û–°–¢–†–û–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê –ó–ê–í–ï–†–®–ï–ù–û ===")

    def save_index(self, base_path: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –≤ —Ñ–∞–π–ª—ã:
        - vocabulary.json - —Å–ª–æ–≤–∞—Ä—å
        - index_metadata.json - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å–∞
        """
        import os
        os.makedirs(base_path, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å (–≤—Å–µ –µ—â–µ –Ω—É–∂–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤)
        vocab_path = f"{base_path}/vocabulary.json"
        self.vocabulary.save_vocabulary(vocab_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'vocabulary_size': self.vocabulary.get_vocabulary_size(),
            'total_documents': self.vocabulary.total_documents,
            'use_vector_db': self.use_vector_db,
            'vector_db_documents': self.vector_storage.get_document_count() if self.vector_storage else 0,
            'index_version': '2.0',
            'description': 'Vector space model index with ChromaDB storage'
        }

        metadata_path = f"{base_path}/index_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        print(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω. –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î: {metadata['vector_db_documents']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    def search(self, query_text: str, preprocessor, top_k: int = 10) -> List[Dict]:
        """
        –£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
        """
        if not self.vector_storage:
            raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

        if not self.tfidf_calculator:
            raise ValueError("TF-IDF –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –≤–∫–ª—é—á–µ–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        if self.all_documents and self.document_selector:
            print("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return self.search_with_selection(query_text, preprocessor, self.all_documents, top_k)
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
            print("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫")
            return self._standard_search(query_text, preprocessor, top_k)

    def _standard_search(self, query_text: str, preprocessor, top_k: int = 10) -> List[Dict]:
        """
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ (–∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)
        """
        processed_terms, query_vector = self.tfidf_calculator.process_query(query_text, preprocessor)

        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–∞: {processed_terms}")
        print(f"–†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞: {len(query_vector)}")

        results = self.vector_storage.search_similar(query_vector, top_k)

        for result in results:
            result['query_terms'] = processed_terms

        return results

    def search_with_selection(self, query_text: str, preprocessor,
                              all_documents: List, top_k: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –æ—Ç–±–æ—Ä–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.document_selector:
            print("‚ö†Ô∏è  –°–µ–ª–µ–∫—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫")
            return self._standard_search(query_text, preprocessor, top_k)

        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º)
        def exact_search(query, documents, k):
            print(f"üîç –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º...")
            
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            doc_map = {doc.doc_id: doc for doc in documents}
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫
            processed_terms, query_vector = self.tfidf_calculator.process_query(query, preprocessor)
            vector_results = self.vector_storage.search_similar(query_vector, k)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            filtered_results = []
            for result in vector_results:
                doc_id = result['metadata']['doc_id']
                if doc_id in doc_map:
                    filtered_results.append(result)
            
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(filtered_results)}")
            return filtered_results

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
        results = self.document_selector.process_search(
            query_text, all_documents, exact_search, top_k
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ—Ä–º–∏–Ω–∞—Ö –∑–∞–ø—Ä–æ—Å–∞
        processed_terms, _ = self.tfidf_calculator.process_query(query_text, preprocessor)
        for result in results:
            result['query_terms'] = processed_terms

        return results

    def analyze_query(self, query_text: str, preprocessor) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –±—ã–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏ –∏—Ö –≤–µ—Å–∞
        """
        if not self.tfidf_calculator:
            return {'error': 'TF-IDF –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω'}

        processed_terms, query_vector = self.tfidf_calculator.process_query(query_text, preprocessor)

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –∏—Ö –≤–µ—Å–æ–≤
        term_analysis = []
        for term in set(processed_terms):
            term_idx = self.vocabulary.get_term_index(term)
            if term_idx != -1:
                weight = query_vector[term_idx]
                df = self.vocabulary.get_document_frequency(term)
                idf = self.tfidf_calculator._calculate_idf(term)
                term_analysis.append({
                    'term': term,
                    'in_vocabulary': True,
                    'document_frequency': df,
                    'idf': round(idf, 4),
                    'weight_in_query': round(weight, 4)
                })
            else:
                term_analysis.append({
                    'term': term,
                    'in_vocabulary': False,
                    'document_frequency': 0,
                    'idf': 0,
                    'weight_in_query': 0
                })

        return {
            'original_query': query_text,
            'processed_terms': processed_terms,
            'term_analysis': term_analysis,
            'query_vector_length': len(query_vector),
            'non_zero_components': sum(1 for x in query_vector if x > 0)
        }

    def get_index_statistics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞"""
        vocab_stats = self.vocabulary.get_statistics()

        stats = {
            **vocab_stats,
            'use_vector_db': self.use_vector_db,
            'vector_db_documents': self.vector_storage.get_document_count() if self.vector_storage else 0,
            'tfidf_vectors_calculated': len(self.tfidf_vectors)
        }

        if self.vector_storage:
            stats.update(self.vector_storage.get_collection_info())

        return stats

    def print_detailed_statistics(self) -> None:
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞"""
        stats = self.get_index_statistics()

        print("\n" + "=" * 60)
        print("–î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–î–ï–ö–°–ê")
        print("=" * 60)
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {stats['vocabulary_size']}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î: {'–î–∞' if stats['use_vector_db'] else '–ù–µ—Ç'}")

        if stats['use_vector_db']:
            print(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: {stats['vector_db_documents']}")
            print(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è: {stats['name']}")
            print(f"–ü–∞–ø–∫–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è: {stats['persist_directory']}")

        print(f"\n–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ç–µ—Ä–º–∏–Ω—ã:")
        for term, freq in stats['most_frequent_terms']:
            print(f"  {term}: {freq} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")


    def get_selection_stats(self) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç–±–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if self.document_selector:
            return self.document_selector.get_selection_statistics()
        return {'document_selector': 'not_used'}